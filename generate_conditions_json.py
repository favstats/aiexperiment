#!/usr/bin/env python3
"""
Generate conditions.json for the Image Gallery and stimuli.json for JSON-based feeds
Run this once after generating images/texts to update the JSON files
"""

import os
import json
from datetime import datetime
from pathlib import Path

# Configuration
IMAGES_DIR = "generated_images"
TEXTS_DIR = "generated_texts"
OUTPUT_FILE = "conditions.json"
STIMULI_OUTPUT_DIR = "circl/data"
STIMULI_OUTPUT_FILE = "stimuli.json"

def parse_condition_id(condition_id):
    """Parse condition_id into components: age, gender, policy, ideology"""
    parts = condition_id.split("_")
    if len(parts) < 4:
        return None
    
    age_group = parts[0]
    gender = parts[1]
    ideology = parts[-1]  # Last part is always ideology
    policy_parts = parts[2:-1]  # Everything between gender and ideology
    policy_issue = "_".join(policy_parts)
    
    # Normalize ideology names to standard format
    ideology_map = {
        "leftist": "left",
        "centrist": "neutral",
        "rightist": "right"
    }
    ideology = ideology_map.get(ideology, ideology)
    
    return {
        "age_group": age_group,
        "gender": gender,
        "policy_issue": policy_issue,
        "ideology": ideology
    }

def generate_stimuli_json(conditions):
    """Generate stimuli.json for the feed-loader system"""
    stimuli_posts = []
    
    for condition in conditions:
        # Create one stimulus post per image-text pair
        images = condition.get("images", [])
        texts = condition.get("texts", [])
        
        # Match images with texts (cycle texts if fewer than images)
        for i, image in enumerate(images):
            text = texts[i % len(texts)] if texts else ""
            
            stimuli_posts.append({
                "id": f"stim_{condition['condition_id']}_{i+1:02d}",
                "type": "stimulus",
                "condition_id": condition["condition_id"],
                "text": text,
                "image": {
                    "src": f"../{IMAGES_DIR}/{condition['condition_id']}/{image}",
                    "alt": f"Political image - {condition['policy_issue']}"
                },
                "author": {
                    "name": None,  # Will be randomized by feed-loader
                    "gender": condition["gender"],
                    "age_group": condition["age_group"],
                    "avatar_url": None  # Will be randomized by feed-loader
                },
                "engagement": None,  # Will be randomized by feed-loader
                "time": None,  # Will be randomized by feed-loader
                "metadata": {
                    "ideology": condition["ideology"],
                    "policy_issue": condition["policy_issue"],
                    "image_index": i
                }
            })
    
    # Build output
    output = {
        "description": "Experimental stimulus posts - generated by Python script",
        "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "posts": stimuli_posts,
        "_schema_notes": {
            "null_fields": "Set any field to null to have it randomized at runtime",
            "author.name": "null = generate Dutch name based on gender/age_group",
            "author.avatar_url": "null = fetch from randomuser.me API based on gender/age",
            "engagement": "null = randomize likes/comments/shares within config ranges",
            "time": "null = randomize within config time_range"
        }
    }
    
    # Ensure output directory exists
    output_dir = Path(STIMULI_OUTPUT_DIR)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Write JSON
    output_path = output_dir / STIMULI_OUTPUT_FILE
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\n✓ Generated {output_path}")
    print(f"  - Total stimulus posts: {len(stimuli_posts)}")
    
    return len(stimuli_posts)


def main():
    conditions = []
    
    # Scan directories
    images_path = Path(IMAGES_DIR)
    if not images_path.exists():
        print(f"Error: {IMAGES_DIR} directory not found")
        return
    
    for condition_dir in sorted(images_path.iterdir()):
        if not condition_dir.is_dir():
            continue
        
        condition_id = condition_dir.name
        parsed = parse_condition_id(condition_id)
        
        if not parsed:
            print(f"  Skipping {condition_id} - couldn't parse")
            continue
        
        # Get images
        images = sorted([
            f.name for f in condition_dir.iterdir()
            if f.suffix.lower() in ['.jpg', '.jpeg', '.png']
        ])
        
        if not images:
            print(f"  Skipping {condition_id} - no images")
            continue
        
        # Get prompt
        prompt_file = condition_dir / "prompt.txt"
        prompt = ""
        if prompt_file.exists():
            prompt = prompt_file.read_text(encoding='utf-8').strip()
        
        # Get texts from generated_texts folder
        texts = []
        texts_dir = Path(TEXTS_DIR) / condition_id
        texts_file = texts_dir / "texts.json"
        if texts_file.exists():
            try:
                with open(texts_file, 'r', encoding='utf-8') as f:
                    texts_data = json.load(f)
                    texts = texts_data.get("posts", [])
            except (json.JSONDecodeError, KeyError) as e:
                print(f"  Warning: Could not read texts for {condition_id}: {e}")
        
        conditions.append({
            "condition_id": condition_id,
            "age_group": parsed["age_group"],
            "gender": parsed["gender"],
            "policy_issue": parsed["policy_issue"],
            "ideology": parsed["ideology"],
            "images": images,
            "texts": texts,
            "prompt": prompt,
            "image_dir": condition_id
        })
    
    # Extract unique filter values
    age_groups = sorted(set(c["age_group"] for c in conditions))
    genders = sorted(set(c["gender"] for c in conditions))
    policy_issues = sorted(set(c["policy_issue"] for c in conditions))
    ideologies = ["left", "neutral", "right"]  # Fixed order
    
    # Count conditions with texts
    conditions_with_texts = sum(1 for c in conditions if c.get("texts"))
    
    # Build output
    output = {
        "meta": {
            "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "total_conditions": len(conditions),
            "images_dir": IMAGES_DIR,
            "texts_dir": TEXTS_DIR
        },
        "filters": {
            "age_groups": age_groups,
            "genders": genders,
            "policy_issues": policy_issues,
            "ideologies": ideologies
        },
        "conditions": conditions
    }
    
    # Write JSON
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\n✓ Generated {OUTPUT_FILE}")
    print(f"  - Total conditions: {len(conditions)}")
    print(f"  - Conditions with texts: {conditions_with_texts}")
    print(f"  - Age groups: {', '.join(age_groups)}")
    print(f"  - Genders: {', '.join(genders)}")
    print(f"  - Policy issues: {len(policy_issues)}")
    print(f"  - Ideologies: {', '.join(ideologies)}")
    
    # Also generate stimuli.json for feed-loader
    generate_stimuli_json(conditions)

if __name__ == "__main__":
    main()















